# Titanic Analysis


The goal of this repo is to showcase the investigative process involved in data science and go beyond short demos to highlight how in deep analysis can reveal structure and insight. Examples are shown in Python, but the goal is to extend the repo to include other languages such as R, to illustrate that it is the critical thinking that is central to data analysis and modelling, rather than particular tools. We also aim to expand the story-telling beyond scripting languages to commonly used dashboards such as Tableau to emphasize the importance of story telling in connecting users with data.

Contents
1. Understanding the data
2. Imputing missing values

Building pipelines (https://scikit-learn.org/stable/modules/compose.html)

https://www.kaggle.com/competitions/titanic

>14,000 teams

At the end of chapters 1-3, we will save a modified version of the training and test data after initial feature engineering (Chapter 1), imputation of missing data (Chapter 2) and advanced feature engineering (Chapter 3). Each modified dataset will allow us to begin the next chapter without repeating the previous steps.

In Chapters 4 onwards, we will produce predictions for testing on Kaggle. 


Ensemble models reflect a failure of intellect - they are the statistical equivalent of diversification in finance. Warren Buffet once reflected that it's much better to invest a lot in a sound profitable business with clear growth potential than to spread money across many businesses. The reason for this is reflected in the Black Swan (Nasseem Talib) as diversification pushes you towards the mean return, which isn't in any way transformative; using ensemble models means that the true model of the data is just one of many whos output is dilutted by the crowd. The widespread use of ensemble models is an indictment of the state of machine learning as a form of numerical sophistry - it reflects the fact that most people aren't interested in the true model, they just want something good enough to run with the crowd.