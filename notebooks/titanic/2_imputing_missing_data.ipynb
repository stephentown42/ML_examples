{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Managing Missing Data\n",
    "\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "Having looked at the data in chapter 1, we can see that some variables of interest such as Age are sometimes missing from the ~20% of the dataset. Our exploration also hinted that this information could be a valuable predictor for whether someone survived and so we might be heavily incentivised to use that feature. \n",
    "\n",
    "In this chapter, we'll answer three questions that are central to any data analysis:\n",
    "1. What's missing?\n",
    "2. Why's it missing?\n",
    "3. How are we going to handle missing data?\n",
    "\n",
    "Let's load the data and get started...</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data = 891 passengers\n",
      "Test data = 418 passengers\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = Path.cwd().parent.parent / 'data/titanic'        # Load data\n",
    "train = pd.read_csv(data_dir / 'train.csv')\n",
    "test = pd.read_csv(data_dir / 'test.csv')\n",
    "\n",
    "print(f\"Training data = {train.shape[0]} passengers\")\n",
    "print(f\"Test data = {test.shape[0]} passengers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. What's missing?\n",
    "\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "In their book <i>Statistical Analysis with Missing Data</i> (3rd Edition, 2020), Little and Rubin distinguish between the patterns of missing data and the mechanisms of missing data. This distinction highlights that it's important to understand where and when data is missing, but also the reasons why data is absent. \n",
    "</span>\n",
    "\n",
    "### 2.1.1. Which columns have missing data\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "For a simple dataset like the titanic files, where each column is supposed to be a distinct measurement, we can start by simply asking which columns have missing data in each dataset. The <span style=\"color: #ffff00\">describe</span> method offers an easy way to do this.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>687</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId  Survived  Pclass Name Sex    Age  SibSp  Parch Ticket  \\\n",
       "train          0.0       0.0     0.0    0   0  177.0    0.0    0.0      0   \n",
       "test           0.0       NaN     0.0    0   0   86.0    0.0    0.0      0   \n",
       "\n",
       "       Fare Cabin Embarked  \n",
       "train   0.0   687        2  \n",
       "test    1.0   327        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1: Use the describe method of the dataframe\n",
    "#   - include='all'     includes both categorical and continuous data\n",
    "#   - .head(1)          gives us just the counts of non-missing data\n",
    "train.describe(include='all').head(1)\n",
    "\n",
    "# To get the number of missing values instead of valid values, use:\n",
    "train.shape[0] - train.describe(include='all').head(1)\n",
    "\n",
    "# We could take this further and contrast counts of missing values for train and test data\n",
    "pd.concat([\n",
    "    train.shape[0] - train.describe(include='all').head(1).rename({'count':'train'}),    \n",
    "    test.shape[0] - test.describe(include='all').head(1).rename({'count':'test'})\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "We have missing rows for <i>Age</i> and <i>Cabin</i> in both test and train data, but we also have rare cases where the port of embarcation (<i>Embarked</i>) is missing from the training data and the <i>Fare</i> paid by one passenger is missing form the test dataset. \n",
    "<br></br>\n",
    "This pattern of missing data already shapes our approach to predicting test data. Either we ignore Age, Cabin and Fare, or we develop a strategy to deal with missing values. The missing Fare data in particular is a nice illustration of how a test dataset can have properties beyond that shown by the training data.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. What denotes a missing value - NaN vs. Numbers?\n",
    "\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "Here, we're assuming that missing values are coded as NaNs, but that might not always be the case. Sometimes  data acqusition systems will return values out of range or zeros. For example, we once had a thermometer that would report body temperatures >5000°C when a thermocouple connector was loose. Fortunately such values are easy to identify when you look at the data's distribution, but it's easy to see how any failures to properly investigate and removing such data could dramatically affect subsequent results.\n",
    "<br></br>\n",
    "If you're designing a data acquisition system or analysis pipeline, it's normally better to use NaNs than other values (zeros, negative numbers, 99 etc.). NaNs have a clear meaning to other people working with the data, as well as to automated systems that might otherwise interpret numeric data wrongly.\n",
    "\n",
    "What about the titanic dataset, are there any indications that other codes have been used to denote missing data?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>80.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>512.3292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass    Age  SibSp  Parch      Fare\n",
       "min          1.0       0.0     1.0   0.42    0.0    0.0    0.0000\n",
       "max        891.0       1.0     3.0  80.00    8.0    6.0  512.3292"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe().loc[['min','max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "Most of the data looks sensible, but what about those people who paid nothing? That seems a bit odd... further inspection of train data shows that only 1 of 15 people paying nothing survived (and 2 people in the test data are unknown). With only 17 people, we can have a closer look through the data to see if there are any further clues:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Andrews, Mr. Thomas Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A36</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Campbell, Mr. William</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>1158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Chisholm, Mr. Roderick Robert Crispin</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Cunningham, Mr. Alfred Fleming</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Frost, Mr. Anthony Wood \"Archie\"</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fry, Mr. Richard</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B102</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Harrison, Mr. William</td>\n",
       "      <td>male</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B94</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Ismay, Mr. Joseph Bruce</td>\n",
       "      <td>male</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B52 B54 B56</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mr. Alfred</td>\n",
       "      <td>male</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LINE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mr. William Cahoone Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LINE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Knight, Mr. Robert J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Leonard, Mr. Lionel</td>\n",
       "      <td>male</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LINE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Parkes, Mr. Francis \"Frank\"</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Parr, Mr. William Henry Marsh</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Reuchlin, Jonkheer. John George</td>\n",
       "      <td>male</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>272</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Tornquist, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LINE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Watson, Mr. Ennis Hastings</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                   Name  \\\n",
       "806          807       0.0       1                 Andrews, Mr. Thomas Jr   \n",
       "466          467       0.0       2                  Campbell, Mr. William   \n",
       "266         1158       NaN       1  Chisholm, Mr. Roderick Robert Crispin   \n",
       "413          414       0.0       2         Cunningham, Mr. Alfred Fleming   \n",
       "481          482       0.0       2       Frost, Mr. Anthony Wood \"Archie\"   \n",
       "815          816       0.0       1                       Fry, Mr. Richard   \n",
       "263          264       0.0       1                  Harrison, Mr. William   \n",
       "372         1264       NaN       1                Ismay, Mr. Joseph Bruce   \n",
       "597          598       0.0       3                    Johnson, Mr. Alfred   \n",
       "302          303       0.0       3        Johnson, Mr. William Cahoone Jr   \n",
       "732          733       0.0       2                   Knight, Mr. Robert J   \n",
       "179          180       0.0       3                    Leonard, Mr. Lionel   \n",
       "277          278       0.0       2            Parkes, Mr. Francis \"Frank\"   \n",
       "633          634       0.0       1          Parr, Mr. William Henry Marsh   \n",
       "822          823       0.0       1        Reuchlin, Jonkheer. John George   \n",
       "271          272       1.0       3           Tornquist, Mr. William Henry   \n",
       "674          675       0.0       2             Watson, Mr. Ennis Hastings   \n",
       "\n",
       "      Sex   Age  SibSp  Parch  Ticket  Fare        Cabin Embarked  \n",
       "806  male  39.0      0      0  112050   0.0          A36        S  \n",
       "466  male   NaN      0      0  239853   0.0          NaN        S  \n",
       "266  male   NaN      0      0  112051   0.0          NaN        S  \n",
       "413  male   NaN      0      0  239853   0.0          NaN        S  \n",
       "481  male   NaN      0      0  239854   0.0          NaN        S  \n",
       "815  male   NaN      0      0  112058   0.0         B102        S  \n",
       "263  male  40.0      0      0  112059   0.0          B94        S  \n",
       "372  male  49.0      0      0  112058   0.0  B52 B54 B56        S  \n",
       "597  male  49.0      0      0    LINE   0.0          NaN        S  \n",
       "302  male  19.0      0      0    LINE   0.0          NaN        S  \n",
       "732  male   NaN      0      0  239855   0.0          NaN        S  \n",
       "179  male  36.0      0      0    LINE   0.0          NaN        S  \n",
       "277  male   NaN      0      0  239853   0.0          NaN        S  \n",
       "633  male   NaN      0      0  112052   0.0          NaN        S  \n",
       "822  male  38.0      0      0   19972   0.0          NaN        S  \n",
       "271  male  25.0      0      0    LINE   0.0          NaN        S  \n",
       "674  male   NaN      0      0  239856   0.0          NaN        S  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([\n",
    "    train[train['Fare'] == 0.0],\n",
    "    test[test['Fare'] == 0.0]           # test data indicated by Survived=NaN\n",
    "]).sort_values(by='Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "<p>If you have background knowledge about the titanic disater, or you've watched the James Cameron film, then some of those names might pop out at you. In particular, Mr Joseph Ismay - a man who paid nothing but had three cabins - what's going on there? Well, he was the managing director of White Star Lines, the company that owned the ship. It's therefore not surprising that he didn't pay to travel, or that he got several rooms. Also listed is Mr Thomas Andrews, the chief designer of the ship.\n",
    "</p><p>\n",
    "Was everyone on this list travelling with the company? Every individual identified was a man, and given that few women were in the workforce in 1912, it makes it more viable that all these individuals had some relevant profession. This is where being able to do more research into a topic is a useful data science skill. Thinking about the real world, rather than just a data table is critical for gaining extra context: Indeed, several people identified above (William Tornquist, William Cahoone Johnson Jr., Alfred Johnson, and Lionel Leonard) were being transported by their employer (American Line).\n",
    "</p><p>\n",
    "The hypothesis that most of zero fee paying individuals were involved in seafaring is also backed up by very low survival rate [1/15 in the training data]. Records on these individuals seem to be patchy as almost all cabin data is missing and the most (53% [8/15]) of these passengers lacked age information (compared to 19% [169/876] of the remaining passengers)\n",
    "</p><p>\n",
    "If so, is the concept of Fare meaningful for these passengers, and if not, should we remove these data, even if they're not technically missing? \n",
    "</p></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_rooms\n",
       "1.0    180\n",
       "2.0     16\n",
       "3.0      6\n",
       "4.0      2\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(by='n_rooms')['Survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Diving deeper into the patterns of missing data\n",
    "\n",
    "As we'll discuss in the next section, we might have reason to believe that data are missing for a reason - i.e. missing not at random (MNAR). For example, if Age and Cabin data was collected from survivors, then that data would not have been available from those who perished. We can if this is true simply by looking at where the missing data are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('Age_missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Why is data missing?\n",
    "\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "We've highlighted \n",
    "\n",
    "\n",
    "Given the similar overlap between age distributions of test and train datasets, it's likely that whatever the cause of missing age data is, it's similar in the two tables.\n",
    "<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. How are we going to handle missing data?\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "several methods for dealing with missing data, either using simple summary statistics or by devising more complex models for predicting missing values based on other features that are available. We'll also cover some statistical models that can handle missing data for you, and what benefits that might offer.\n",
    "\n",
    "The first thing we'll do is note whether the variable of interest is missing at all, as this can be an important marker that we can use to indicate to ML models that the input data isn't original. \n",
    "\n",
    "Note: This chapter is an exploration of some imputation approaches - see also:\n",
    " * [sklearn.impute](https://scikit-learn.org/stable/modules/impute.html) \n",
    " * [impyute](https://impyute.readthedocs.io/en/master/)\n",
    "\n",
    "\n",
    "Credit: The code written below was developed in collaboration with ChatGPT from OpenAI\n",
    "<span>\n",
    "### 2.3.1. Preparing the data ahead of imputation\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "The fact that data is missing may be interesting itself, and even indicative of the target variable (e.g. if ages of people who died couldn't be recovered). This is particularly the case for features with lots of missing data, such as Age and Class, and so for each of these features, we add a column noting whether the feature is missing or not:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['age_missing'] = train['Age'].isna()\n",
    "test['age_missing'] = test['Age'].isna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "<p>Looking ahead to methods of imputation, some approaches such as Regression Imputation, may only take in numeric data. If we want to predict missing data based on categorical information (e.g. predicting Age from Class), we must create numerical values representing each value of the category (e.g. Male=0, Female=1 etc.).\n",
    "</p>\n",
    "<p>Note that in the code below, I've used the map method with a specified dictionary of values to make the coding of variables explicit and the numeric values easier to interpret. Mapping in this way also ensures consistency between parallel operations on test and train datasets, while also ensuring that any future values of the source feature that aren't in the map do not receive values. \n",
    "</p>\n",
    "<p>This latter point can be important if we don't know the distribution of feature categories in the test dataset - e.g. if males and females were in the training data, but only females were in the test data and we used a method such as <i>factorize</i>, females would encoded as 1 in the training data, and 0 in the test data. That could be a major bug! (Not that I've ever done that...)\n",
    "<p></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_map = {'male':0,'female':1}\n",
    "train['gender_numeric'] = train['Sex'].map(gender_map)\n",
    "test['gender_numeric'] = test['Sex'].map(gender_map)\n",
    "\n",
    "embark_map = {'C':1,'Q':2,'S':3}\n",
    "train['embarked_numeric'] = train['Embarked'].map(embark_map)\n",
    "test['embarked_numeric'] = test['Embarked'].map(embark_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3.2. Methods for replacing missing data\n",
    "\n",
    "\n",
    "#### 2.3.1.1. Deletion\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "This involves simply removing any rows or columns that contain missing data. This method is simple and easy to implement, but it can also reduce the sample size and potentially bias the results if the missing data is not missing completely at random.\n",
    "<br></br>\n",
    "Removing observations with missing data also reduces the size of our training set, which could be important in building more sophisticated models in the future. If we're running a business, it might also be very expensive to remove datapoints that we've spent money acquiring. Deletion is therefore also inefficient.\n",
    "<br></br>\n",
    "A final problem with deleting data is that it doesn't provide a useful strategy for making predictions on the test set. What happens if we encounter missing data in testing or deployment? If we just delete missing data, then either we can't base predictions a feature that contains missing data (which is highly undesirable) or we have to skip test cases where data is missing (which will hurt performance badly).\n",
    "<br></br>\n",
    "For the reasons above, we will avoid removing missing data. The code below simply illustrates how it might be done.\n",
    "<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.drop(train.index[train[\"age_missing\"]])\n",
    "# test = test.drop(test.index[test[\"age_missing\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.3.2.2. Mean, median or mode imputation\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    " This involves replacing the missing data with the mean or median value of the non-missing data in the same column. This can be useful if the missing data is missing at random and if the data is normally distributed. However, it can also distort the distribution of the data and potentially introduce bias if the data is not normally distributed or if the missing data is not missing at random.\n",
    "<br></br>\n",
    " As shown in Chapter 1, most of the input features for the titanic dataset are not normally distributed and so mean imputation is unsuitable for an in-depth analysis. However in the case of Embarked, we have only two missing datapoints and the data type is categorical, that imputation using the most common port of embarcation (i.e. the mode) seems reasonable.</span>\n",
    " <br></br>\n",
    " <span style=\"font-size:14px;font-family:Times New Roman;line-height:1.2;color:#aaaaaa\">\n",
    " Side note: Often people will use mean imputation just to get a solution working, for example in interviews or courses where time is limited. Beyond the biological situations in which most of statistics was developed, the assumption of normality in real-world scenarios rarely met - see Nassem Talib's book on The Black Swan for more in-depth discussion and the  implications of fat-tailed distributions.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with mean \n",
    "# train.fillna(train.mean(), inplace=True)\n",
    "# test.fillna(test.mean(), inplace=True)\n",
    "\n",
    "# Replace with median \n",
    "# train.fillna(train.mean(), inplace=True)\n",
    "# test.fillna(test.mean(), inplace=True)\n",
    "\n",
    "# Replace with mode \n",
    "most_common_embark = train['Embarked'].mode()[0]\n",
    "train['Embarked'].fillna(most_common_embark, inplace=True)\n",
    "# (note there is no missing data for Embark in the test dataset - see Section 2.1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.3. Regression Imputation\n",
    "\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "<p>\n",
    "This involves using a regression model to predict the missing values based on the non-missing values in the same row or other relevant variables. This can be useful if the missing data is not missing at random and if there is a strong relationship between the missing and non-missing values. However, it can also introduce bias and error if the regression model is misspecified or if there is not a strong relationship between the variables.\n",
    "</p><p>\n",
    "In the case of the titanic dataset, we are somewhat limited in the predictors available for imputation. We can get an idea of the relationships between continuous variables by looking at the correlation matrix. Note that here we're considering the factorized version of categorical variables such as gender and class (and class has ordinal implications in that 1st is different from 2nd class in at least the same \"direction\" as 2nd is different from 3rd class)\n",
    "</p><p>\n",
    "If we try to impute values this way, we'll find that a simple regression model does a bad job and predicts impossible values, such as negative ages.\n",
    "</p></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[['Age','SibSp','Fare','PassengerId','gender_numeric','Pclass','Parch']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that uses the model to make predictions for missing values\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # Build model from available data\n",
    "# predictors = ['SibSp','Fare','gender_numeric','Pclass','Parch']\n",
    "\n",
    "# X = train[predictors].dropna().values\n",
    "# y = train[\"Age\"].dropna().values\n",
    "\n",
    "# model = LinearRegression().fit(X, y)\n",
    "\n",
    "# idx = train[train[\"age_missing\"]].index\n",
    "# train.loc[idx, \"Age\"] = model.predict(train.loc[idx, predictors]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.3.2.4. K-Nearest Neighbors Imputation\n",
    "\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "<p>\n",
    "This involves using the values of the k-nearest neighbors to the missing data point to impute the missing value. This can be useful if the data is missing at random and if the missing data is similar to the values of its neighboring points. However, it can also introduce bias and error if the data is not missing at random or if the nearest neighbors are not representative of the missing data point.\n",
    "</p><p>\n",
    "These are just a few examples of methods for replacing missing data. There are many other methods and techniques that can be used, and the appropriate method to use depends on the specific context and characteristics of the missing data. It is important to carefully evaluate the missing data and consider the potential implications of different methods before deciding on a course of action.\n",
    "</p></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors = 3)\n",
    "\n",
    "# Select columns to use for impute\n",
    "columns_for_impute = ['Pclass','SibSp', 'Parch', 'Fare', 'gender_numeric', 'embarked_numeric','Age']\n",
    "\n",
    "# Fit the imputer to the data\n",
    "imputer.fit(train[columns_for_impute])\n",
    "\n",
    "# Transform the data\n",
    "imputed_data = imputer.transform(train[columns_for_impute])\n",
    "\n",
    "# Create a new dataframe with the imputed data\n",
    "imputed_df = pd.DataFrame(imputed_data, columns = columns_for_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.5. Neural Networks\n",
    "\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "Given the hype, it's important to remember that a neural net isn't magic - it can't find relationships if they don't exist. Also, if those relationships are very complex, we may not have enough data to find them.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Create a sample dataset with missing values\n",
    "data = np.array([[1, 2, np.nan],\n",
    "                 [3, 4, 5],\n",
    "                 [6, np.nan, 7],\n",
    "                 [np.nan, 8, 9]])\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(10, input_shape=(3,)),\n",
    "    keras.layers.Dense(10),\n",
    "    keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(data, data, epochs=100)\n",
    "\n",
    "# Use the trained model to impute missing values\n",
    "imputed_data = model.predict(data)\n",
    "\n",
    "# Print the imputed dataset\n",
    "print(imputed_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Further Considerations\n",
    "\n",
    "### 2.4.1. Documenting Missing Data\n",
    "\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "We began this chapter by discussing how to find missing data and think about the reasons why some datapoints might be absent. While the titanic dataset is useful for illustrating the general flow of data science workflows, it should be noted that it's best practice to document the sources and reasons for missing data at the point of data collection. \n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "### 2.4.2. Approaches that are robust to missing data\n",
    "\n",
    "- Generalized linear mixed models\n",
    "- XGBoost (Sparsity Aware Split Finding)\n",
    "\n",
    "### 2.4.3. Preventing Data Leakage: Never merge the train and test data\n",
    "\n",
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "\n",
    "When imputing missing data, one might think that we should combine data from the training and test datasets, so that we can maximise our ability to estimate missing values. However, if we were to merge the two datasets, we would potentially risk **data leakage** that might undermine the model we're training. \n",
    "\n",
    "A simple example using regression imputation shows you why: Here we simulate a simple experiment in which age is correlated with ticket fare, but the correlation coefficients and ranges of fares in test and train data are different:\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_passengers, max_age = 200, 80\n",
    "\n",
    "train_age = np.random.rand(n_passengers) * max_age\n",
    "test_age = np.random.rand(n_passengers) * (max_age/2)              # test subjects are half as old \n",
    "\n",
    "train_fare = (train_age * 10) + (25*np.random.rand(n_passengers))   # y = a + bx + noise\n",
    "test_fare = (test_age * 5) + (25*np.random.rand(n_passengers))      \n",
    "\n",
    "# mask a subset of ages (data has no order)\n",
    "proportion_missing = 0.3\n",
    "train_age[:int(n_passengers*proportion_missing)] = np.nan   \n",
    "fares_for_missing_data = train_fare[:int(n_passengers*proportion_missing)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values using linear regression with only training data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = train_fare[~np.isnan(train_age)].reshape(-1,1)\n",
    "y = train_age[~np.isnan(train_age)].reshape(-1,1)\n",
    "\n",
    "training_only = LinearRegression().fit(X, y)\n",
    "trained_only_prediction = training_only.predict(fares_for_missing_data.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now impute values using linear regression on merged test and training data\n",
    "X_merged = np.vstack((X, test_fare.reshape(-1, 1)))\n",
    "y_merged = np.vstack((y, test_age.reshape(-1, 1)))\n",
    "\n",
    "merged_model = LinearRegression().fit(X_merged, y_merged)\n",
    "merged_prediction = merged_model.predict(fares_for_missing_data.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:14px;font-family: Arial;line-height:1.5\">\n",
    "When we plot the results in the cell below, we can see that the imputed ages of subjects is very strongly affected by the merging of train and test datasets. If we were then to build our survival model based on the ages imputed after merging, that model would also be influenced by the test data. In deployment, such a model may generalize poorly because it can no longer rely on the leaked information that allowed it to perform well on the test data.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(train_fare, train_age, marker='.', color='c', alpha=0.5, label='Train (Observed)')\n",
    "plt.scatter(test_fare, test_age, marker='.', color='orange', alpha=0.5, label='Test (Observed)')\n",
    "\n",
    "plt.scatter(\n",
    "    train_fare[np.isnan(train_age)], \n",
    "    trained_only_prediction, \n",
    "    marker='x', s=5, color='b', label='Train (Imputed)')\n",
    "\n",
    "plt.scatter(\n",
    "    train_fare[np.isnan(train_age)], \n",
    "    merged_prediction, \n",
    "    marker='x', s=5, color='r', label='Merged (Imputed)')\n",
    "\n",
    "plt.ylabel('Age (Years)')\n",
    "plt.xlabel('Fare (£)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.4.2. Multiple Imputation\n",
    "\n",
    "(Credit: Sklearn.Impute Documentation)\n",
    "\n",
    "In the statistics community, it is common practice to perform multiple imputations, generating, for example, *m* separate imputations for a single feature matrix. Each of these *m* imputations is then put through the subsequent analysis pipeline (e.g. feature engineering, clustering, regression, classification). The *m* final analysis results (e.g. held-out validation errors) allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Note that imputing missing data is not always a valid approach - particularly in research science, where the primacy of data is critical (and single data points can cost thousands of dollars to obtain). Here, it may be better to have no data and be clear that the absence exists than it is to fill in missing data based on assumptions or estimates that are ultimately based on the prejudice of the investigator.\n",
    "\n",
    "However, i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 16:21:59) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b730ead757b5d9b842526340519d98f65a574c00873e9637238694ddd8f9228"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
