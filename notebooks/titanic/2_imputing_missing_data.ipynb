{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Managing Missing Data\n",
    "\n",
    "Having looked at the data in chapter 1, we can see that some variables of interest such as Age are sometimes missing from the ~20% of the dataset. Our exploration also hinted that this information could be a valuable predictor for whether someone survived and so we might be heavily incentivised to use that feature. \n",
    "\n",
    "In this chapter, we'll discuss several methods for dealing with missing data, either using simple summary statistics or by devising more complex models for predicting missing values based on other features that are available. We'll also cover some statistical models that can handle missing data for you, and what benefits that might offer.\n",
    "\n",
    "The first thing we'll do is note whether the variable of interest is missing at all, as this can be an important marker that we can use to indicate to ML models that the input data isn't original. \n",
    "\n",
    "Note: This chapter is an exploration of some imputation approaches - see also:\n",
    " * [sklearn.impute](https://scikit-learn.org/stable/modules/impute.html) \n",
    " * [impyute](https://impyute.readthedocs.io/en/master/)\n",
    "\n",
    "\n",
    "Credit: The code written below was developed in collaboration with ChatGPT from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dir = Path.cwd().parent.parent / 'data/titanic'\n",
    "train = pd.read_csv(data_dir / 'train.csv')\n",
    "test = pd.read_csv(data_dir / 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Data Preprocessing\n",
    "\n",
    "Note if data is missing (this can also be done using the MissingIndicator from sklearn.impute)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Understanding what's missing\n",
    "\n",
    "In their book *Statistical Analysis with Missing Data* (3rd Edition, 2020), Little and Rubin distinguish between the patterns of missing data and the mechanisms of missing data. This distinction highlights that it's important to understand where and when data is missing, but also the reasons why data is absent. \n",
    "\n",
    "Let's start off by asking the obvious, which columns have missing data in each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>177.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>687.0</td>\n",
       "      <td>327.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          train   test\n",
       "Age       177.0   86.0\n",
       "Cabin     687.0  327.0\n",
       "Embarked    2.0    NaN\n",
       "Fare        NaN    1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_columns_with_missing_data(df, name:str) -> None:\n",
    "    missing_vals = df.isnull().sum()\n",
    "    return pd.DataFrame(missing_vals[missing_vals > 0], columns=[name])\n",
    "\n",
    "pd.concat([\n",
    "    get_columns_with_missing_data(train, 'train'),\n",
    "    get_columns_with_missing_data(test, 'test')\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have missing rows for Age and Cabin in both test and train data, but we also have rare cases where the port of embarcation is missing from the training data and the fare paid by one passenger is missing form the test dataset. (This again illustrates how the titanic project has a few of these awkard properties that reflect the challenges of real-world analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['age_missing'] = train['Age'].isna()\n",
    "test['age_missing'] = test['Age'].isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Do not merge the train and test data\n",
    "\n",
    "When imputing missing data, one might think that we should combine data from the training and test datasets, so that we can maximise our ability to estimate missing values. However, if we were to merge the two datasets, we would potentially risk **data leakage** that might undermine the model we're training. \n",
    "\n",
    "A simple example using regression imputation shows you why: Here we simulate a simple experiment in which age is correlated with ticket fare, but the correlation coefficients and ranges of fares in test and train data are different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_passengers, max_age = 200, 80\n",
    "\n",
    "train_age = np.random.rand(n_passengers) * max_age\n",
    "test_age = np.random.rand(n_passengers) * (max_age/2)              # test subjects are half as old \n",
    "\n",
    "train_fare = (train_age * 10) + (25*np.random.rand(n_passengers))   # y = a + bx + noise\n",
    "test_fare = (test_age * 5) + (25*np.random.rand(n_passengers))      \n",
    "\n",
    "# mask a subset of ages (data has no order)\n",
    "proportion_missing = 0.3\n",
    "train_age[:int(n_passengers*proportion_missing)] = np.nan   \n",
    "fares_for_missing_data = train_fare[:int(n_passengers*proportion_missing)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values using linear regression with only training data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = train_fare[~np.isnan(train_age)].reshape(-1,1)\n",
    "y = train_age[~np.isnan(train_age)].reshape(-1,1)\n",
    "\n",
    "training_only = LinearRegression().fit(X, y)\n",
    "trained_only_prediction = training_only.predict(fares_for_missing_data.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now impute values using linear regression on merged test and training data\n",
    "X_merged = np.vstack((X, test_fare.reshape(-1, 1)))\n",
    "y_merged = np.vstack((y, test_age.reshape(-1, 1)))\n",
    "\n",
    "merged_model = LinearRegression().fit(X_merged, y_merged)\n",
    "merged_prediction = merged_model.predict(fares_for_missing_data.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we plot the results in the cell below, we can see that the imputed ages of subjects is very strongly affected by the merging of train and test datasets. If we were then to build our survival model based on the ages imputed after merging, that model would also be influenced by the test data. In deployment, such a model may generalize poorly because it can no longer rely on the leaked information that allowed it to perform well on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(train_fare, train_age, marker='.', color='c', alpha=0.5, label='Train (Observed)')\n",
    "plt.scatter(test_fare, test_age, marker='.', color='orange', alpha=0.5, label='Test (Observed)')\n",
    "\n",
    "plt.scatter(\n",
    "    train_fare[np.isnan(train_age)], \n",
    "    trained_only_prediction, \n",
    "    marker='x', s=5, color='b', label='Train (Imputed)')\n",
    "\n",
    "plt.scatter(\n",
    "    train_fare[np.isnan(train_age)], \n",
    "    merged_prediction, \n",
    "    marker='x', s=5, color='r', label='Merged (Imputed)')\n",
    "\n",
    "plt.ylabel('Age (Years)')\n",
    "plt.xlabel('Fare (Â£)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some methods, it may be useful to include categorical data in the imputation. However string values are not always compatible and so we create numerical values representing each value of the category (e.g. Male=0, Female=1 etc.). We apply the factorize function to data in both the train and test set, so that we can impute missing data when making final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['gender_numeric'], _ = train['Sex'].factorize()\n",
    "test['gender_numeric'], _ = test['Sex'].factorize()\n",
    "\n",
    "train['embarked_numeric'], _ = train['Embarked'].factorize()\n",
    "test['embarked_numeric'], _ = test['Embarked'].factorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Methods for replacing missing data\n",
    "\n",
    "### Deletion\n",
    "\n",
    "This involves simply removing any rows or columns that contain missing data. This method is simple and easy to implement, but it can also reduce the sample size and potentially bias the results if the missing data is not missing completely at random.\n",
    "\n",
    "If we were to get rid of missing values for Age data, we could still train a model that included Age as a predictor, so long as the testing data did not contain missing values. However we know that the training data does contain missing data (and even if we didn't, we might not want to limit ourselves to the assumption that all data was present). For these reasons, we will comment out the code to delete missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.drop(train.index[train[\"age_missing\"]])\n",
    "test = test.drop(test.index[test[\"age_missing\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Mean/median imputation\n",
    " This involves replacing the missing data with the mean or median value of the non-missing data in the same column. This can be useful if the missing data is missing at random and if the data is normally distributed. However, it can also distort the distribution of the data and potentially introduce bias if the data is not normally distributed or if the missing data is not missing at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with mean \n",
    "# train.fillna(train.mean(), inplace=True)\n",
    "# test.fillna(test.mean(), inplace=True)\n",
    "\n",
    "# Replace with median \n",
    "# train.fillna(train.mean(), inplace=True)\n",
    "# test.fillna(test.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression imputation\n",
    "\n",
    "This involves using a regression model to predict the missing values based on the non-missing values in the same row or other relevant variables. This can be useful if the missing data is not missing at random and if there is a strong relationship between the missing and non-missing values. However, it can also introduce bias and error if the regression model is misspecified or if there is not a strong relationship between the variables.\n",
    "\n",
    "In the case of the titanic dataset, we are somewhat limited in the predictors available for imputation. We can get an idea of the relationships between continuous variables by looking at the correlation matrix. Note that here we're considering the factorized version of categorical variables such as gender and class (and class has ordinal implications in that 1st is different from 2nd class in at least the same \"direction\" as 2nd is different from 3rd class)\n",
    "\n",
    "If we try to impute values this way, we'll find that a simple regression model does a bad job and predicts impossible values, such as negative ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[['Age','SibSp','Fare','PassengerId','gender_numeric','Pclass','Parch']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that uses the model to make predictions for missing values\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # Build model from available data\n",
    "# predictors = ['SibSp','Fare','gender_numeric','Pclass','Parch']\n",
    "\n",
    "# X = train[predictors].dropna().values\n",
    "# y = train[\"Age\"].dropna().values\n",
    "\n",
    "# model = LinearRegression().fit(X, y)\n",
    "\n",
    "# idx = train[train[\"age_missing\"]].index\n",
    "# train.loc[idx, \"Age\"] = model.predict(train.loc[idx, predictors]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### K-nearest neighbors imputation\n",
    "\n",
    "This involves using the values of the k-nearest neighbors to the missing data point to impute the missing value. This can be useful if the data is missing at random and if the missing data is similar to the values of its neighboring points. However, it can also introduce bias and error if the data is not missing at random or if the nearest neighbors are not representative of the missing data point.\n",
    "\n",
    "These are just a few examples of methods for replacing missing data. There are many other methods and techniques that can be used, and the appropriate method to use depends on the specific context and characteristics of the missing data. It is important to carefully evaluate the missing data and consider the potential implications of different methods before deciding on a course of action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors = 3)\n",
    "\n",
    "# Select columns to use for impute\n",
    "columns_for_impute = ['Pclass','SibSp', 'Parch', 'Fare', 'gender_numeric', 'embarked_numeric','Age']\n",
    "\n",
    "# Fit the imputer to the data\n",
    "imputer.fit(train[columns_for_impute])\n",
    "\n",
    "# Transform the data\n",
    "imputed_data = imputer.transform(train[columns_for_impute])\n",
    "\n",
    "# Create a new dataframe with the imputed data\n",
    "imputed_df = pd.DataFrame(imputed_data, columns = columns_for_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[columns_for_impute].head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "Given the hype, it's important to remember that a neural net isn't magic - it can't find relationships if they don't exist. Also, if those relationships are very complex, we may not have enough data to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Create a sample dataset with missing values\n",
    "data = np.array([[1, 2, np.nan],\n",
    "                 [3, 4, 5],\n",
    "                 [6, np.nan, 7],\n",
    "                 [np.nan, 8, 9]])\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(10, input_shape=(3,)),\n",
    "    keras.layers.Dense(10),\n",
    "    keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(data, data, epochs=100)\n",
    "\n",
    "# Use the trained model to impute missing values\n",
    "imputed_data = model.predict(data)\n",
    "\n",
    "# Print the imputed dataset\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.?. Multiple Imputation\n",
    "\n",
    "(Credit: Sklearn.Impute Documentation)\n",
    "\n",
    "In the statistics community, it is common practice to perform multiple imputations, generating, for example, *m* separate imputations for a single feature matrix. Each of these *m* imputations is then put through the subsequent analysis pipeline (e.g. feature engineering, clustering, regression, classification). The *m* final analysis results (e.g. held-out validation errors) allow the data scientist to obtain understanding of how analytic results may differ as a consequence of the inherent uncertainty caused by the missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Note that imputing missing data is not always a valid approach - particularly in research science, where the primacy of data is critical (and single data points can cost thousands of dollars to obtain). Here, it may be better to have no data and be clear that the absence exists than it is to fill in missing data based on assumptions or estimates that are ultimately based on the prejudice of the investigator.\n",
    "\n",
    "However, i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b730ead757b5d9b842526340519d98f65a574c00873e9637238694ddd8f9228"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
